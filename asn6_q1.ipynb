{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "asn6-q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BronsonSchultz/487_A6/blob/main/asn6_q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeIvKapTDKw1"
      },
      "source": [
        "# Import Libraries "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54svCJ-vDKw_"
      },
      "source": [
        "Import all the required Tensorflow and Keras libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWCODHE3DKxB"
      },
      "source": [
        "# THIS VERSION IS FOR TENSORFLOW v2.0 which has keras embedded\n",
        "# Note: Multibackend Keras is being discontinued after version 2.3.5.\n",
        "\n",
        "# These are all the imports we will need.  You shouldn't need anything else.\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import regularizers \n",
        "\n",
        "import os as os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset sizes\n",
        "num_train_images = 23000\n",
        "num_validation_images = 2000\n",
        "num_test_images = 399\n",
        "\n",
        "# Model Hyperparameters\n",
        "input_image_size = 64\n",
        "batch_size = 32\n",
        "num_training_epochs = 40\n",
        "learning_rate = 0.001\n",
        "l2_lambda = 0.01\n",
        "\n",
        "# Parameters derived from hyperparameters\n",
        "training_steps_per_epoch = int(num_train_images/batch_size)\n",
        "validation_steps_per_epoch = int(num_validation_images/batch_size)\n",
        "testing_steps_per_epoch = num_test_images/batch_size\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIUltlrBDKxC"
      },
      "source": [
        " # Step 1: Design the CNN architecture. After designing the architecture, print it in iPython Notebook. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aydhrl6KDKxD",
        "outputId": "0f6b79b7-e3ad-4a47-8f57-6ca2188e98ee"
      },
      "source": [
        "# WRITE THE CODE DEFINING THE NETWORK ARCHITECTURE HERE (Sec. 2.2 in the assignment PDF)\n",
        "\n",
        "# Initialize the CNN using the Sequential() function from keras.models and assign it to the variable 'model'.\n",
        "model = Sequential([\n",
        "                    # Add a convolution layer with 8 feature maps of shape (3,3), and input_shape=(64,64,3).\n",
        "                    # Use 'relu' for the activation function.\n",
        "                    Conv2D(filters=8, kernel_size=(3,3), activation=\"relu\", input_shape=(64,64,3)),\n",
        "\n",
        "                    # Add a max pooling layer, with a pool_size of (2,2)\n",
        "                    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "                    # Add a three more pairs of convolution and max pooling layers.  Use 32, 64, and 128 feature maps, respectively,\n",
        "                    # for each successive convolutional layer.  Use pool_size of (2,2) for every max pooling lyaer.\n",
        "                    # Don't specify input_shape for the convolutional layer this time since it can be inferred from the previous layer.\n",
        "                    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
        "                    MaxPooling2D(pool_size=(2,2)),\n",
        "                    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "                    MaxPooling2D(pool_size=(2,2)),\n",
        "                    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
        "                    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "                    # Add a flattening layer.\n",
        "                    Flatten(),\n",
        "\n",
        "                    # Add a fully connected layer with 128 units and the RELU activation function. This\n",
        "                    # will be a hidden layer.  Use the L2 kernel regularizer with a lambda of l2_lambda (defined above).\n",
        "                    Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "                    \n",
        "                    # Add a fully connected layer with 1 unit and the sigmoid activation function.  This\n",
        "                    # will be the output layer.\n",
        "                    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Create an Adam optimizer object with learning rate equal to learning_rate (defined above);\n",
        "# call Adam() with the parameter lr=learning_rate and assign it to the variable 'opt'.\n",
        "\n",
        "opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "# Compile the CNN using the compile() method.  Use the 'adam' optimizer (optimizer=opt), and the \n",
        "# 'binary_crossentropy' loss function.  Use the parameter metrics=['accuracy'].  \n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#Print a Summary of the Architecture using the summary() method.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 8)         224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 29, 29, 32)        2336      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 160,705\n",
            "Trainable params: 160,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWmApyAeDKxE"
      },
      "source": [
        "# Step 2: Load Images and Prepare the Network for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O70gnKiyDKxE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "0505fce0-9fbe-47a9-b802-16b439837a2e"
      },
      "source": [
        "#Train and Test ImageDataGenerator code\n",
        "\n",
        "# Create the ImageDataGenerator() for the training data.  \n",
        "# We want to re-scale the image pixel data to the range 0.0 to 1.0 by dividing by 255.0.  \n",
        "# Also We want to use real-time data augmentation allowing horizontal flips, \n",
        "# and a modest amount of zooming and shearing. \n",
        "# Thus, use the paramters rescale=1./255, shear_range=0.1, zoom_range=0.1, horizontal_flip=True.\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255, shear_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Create the ImageDataGenerator() for the validation data.  It doesn't need any data augmentation,\n",
        "# only the scaling of the image pixel data, so omit everything but the rescale parameter.\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Load the training set using train_datagen.flow_from_directory().  Use\n",
        "# target_size = (input_image_size, input_image_size), batch_size=batch_size, and class_mode = 'binary'.\n",
        "\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\"/content/487_A6/train/train\", target_size=(input_image_size,input_image_size), batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Load the validation dataset using validation_datagen.flow_from_directory(). \n",
        "# Use the same parameters as above.\n",
        "\n",
        "valid_data = valid_data_gen.flow_from_directory(\"/content/487_A6/valid/valid\", target_size=(input_image_size, input_image_size), batch_size=batch_size, class_mode='binary')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7ff771b582b7>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    [your code here]\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PzbG6s9DKxF"
      },
      "source": [
        "# Step 3: Train the CNN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HhtR0IrnDKxF"
      },
      "source": [
        "# Train the CNN using the fit() function of your CNN.  The first arguments hould be your training set generator from step 2.\n",
        "#\n",
        "# Use steps_per_epoch=training_steps_per_epoch, epochs=num_training_epochs, validation_data=validation_set,\n",
        "# and validation_steps=validation_steps_per_epoch, and verbose=1.\n",
        "#\n",
        "# training_steps_per_epoch defined above as num_training_images / batch_size.  Since each step will process\n",
        "# batch_size images, this number of steps will run through the training set exactly once per eopch.  \n",
        "# This calculation results in steps_per_epoch = num_training_images / batch_size = 718 batches to make one pass through the training set.\n",
        "# This is, of course, done num_training_epochs times.\n",
        "#\n",
        "# Similarly, validation_steps_per_epoch is defined as num_validation_images / batch_size.  Again, this causes\n",
        "# one validation pass to run through the validation set exactly once.  \n",
        "# This calculation results in validation_steps = 2000/32 = 63 batches to make one pass through the validation set.\n",
        "\n",
        "# You should see the loss function and the accuracy improving quite a bit after each of the \n",
        "# first few epochs.  Then it should slow down.  If you allow it to run for more than 40 epochs \n",
        "# you probably won't see much additional improvement (for me, after 55 epochs it was no better \n",
        "# than it was after 40 epochs).\n",
        "\n",
        "# Expect a pause at the end of each epoch as it classifies images from the validation set.  The validation accuracy \n",
        "# should track the training accuracy fairly well.\n",
        "\n",
        "# This will take a while - maybe 2-3 hours.  On my computer (which is a very new\n",
        "# 8-core macbook pro) it took about 106s seconds per epoch training on the CPU (not GPU).  \n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    epochs=num_training_epochs,\n",
        "                    validation_data=valid_data,\n",
        "                    validation_steps=validation_steps_per_epoch,\n",
        "                    verbose=1,\n",
        "                    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHOIDOQDKxG"
      },
      "source": [
        "# Step 4: Save the model and weights for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu7ZuWV3DKxH"
      },
      "source": [
        "# Save the model using the save() method of the CNN model.\n",
        "\n",
        "# You don't have to code anything here except to change the filename to something suitable for you\n",
        "# and then run this block.\n",
        "\n",
        "model.save('Cat-Dog-64x64-4layer-maxpooling.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrYXdoIDKxH"
      },
      "source": [
        "# Step 5: Plot the training and Validation Loss/Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjJY8BpaDKxH"
      },
      "source": [
        "# Use the history of the training and validation accuracy and loss \n",
        "# from the history object returned by model.fit_generator().  Plot\n",
        "# two graphs as described in section 3.6 of the assignment PDF.  \n",
        "#\n",
        "# e.g. history.history['accuracy'] is an array of the training accuracy for each epoch,\n",
        "plt.plot(history.history['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjpo9xVnDKxI"
      },
      "source": [
        "## Interpreting the Graphs\n",
        "\n",
        "Answer the following questions directly in this block (if you can't produce the graphs yourself, answer based on the sample outputs in the assignment PDF):\n",
        "\n",
        "1. The graphs suggest there may be a problem with our network.  What is it?\n",
        "\n",
        "[Answer here]\n",
        "\n",
        "2. What is it about the graphs that tells you that this is a problem?\n",
        "\n",
        "[Answer here]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq6dZBN0DKxI"
      },
      "source": [
        "# Step 6: Predict Doc/Cat using the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPne8YzrDKxI"
      },
      "source": [
        "# Start by loading your model from disk that you previously saved so you don't have to run through\n",
        "# the rest of the notebook just to test the model.  Change the filename to the same filename you used in Step 4.\n",
        "model = load_model('/Users/mark/Dropbox/CMPT487/assignments-python/asn6/solution/Cat-Dog-64x64-4layer-maxpooling.h5')\n",
        "\n",
        "# Now, create a data generator for the test images, just like for the training and validation sets.  \n",
        "\n",
        "# We still want to rescale the intensities by dividing by 255 because pre-processing of test images must match\n",
        "# the preprocessing of training and test images.  But we do not want any data augmentation so instantiate\n",
        "# the ImageDataGenerator in the same way you did for the validation set.\n",
        "\n",
        "# When calling flow_from_directory() we need class_mode = None, and shuffle = False so that we don't re-order the data.  \n",
        "# Use the same target_size and batch_size as before.\n",
        "\n",
        "test_datagen = [your code here]\n",
        "\n",
        "generator = test_datagen.flow_from_directory( [your code here] )\n",
        "\n",
        "# Now call model.predict_generator().\n",
        "# The parameter to model.predict_generator() should be the 'generator' object, above.\n",
        "\n",
        "probabilities = model.predict_generator( [your code here] )\n",
        "\n",
        "# take the 'probabilities' array, above, and convert to an array y_pred which is equal to 1 if \n",
        "# probabilities > .5, and 0 otherwise.  Also use np.squeeze() to remove the second dimension \n",
        "# of the array (which has length 1 and is not needed) so that y_pred has shape (499,), i.e. a 1-D array.  \n",
        "# If you've done this right, your y_pred array should be exactly the same shape as generator.classes.\n",
        "\n",
        "y_pred = [your code here]\n",
        "\n",
        "# Compare y_pred to generator.classes, which are the correct class labels, and compute and print out the classification accuracy.\n",
        "# You should expect a classification rate of around 85%.\n",
        "\n",
        "accuracy = [your code here]\n",
        "print('The classification rate is', accuracy)\n",
        "                                        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSmoMAF3DKxJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}